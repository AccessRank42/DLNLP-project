{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3f8df10b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading wordvecs to ./data\n",
            "File glove.840B.300d.zip successfully loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\temp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will download SQuAD datasets to ./data\n",
            "Will put preprocessed SQuAD datasets in ./data\n",
            "File train-v1.1.json successfully loaded\n",
            "Train data has 87599 examples total\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  97\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1173\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  23\n",
            "Processed 86306 examples of total 87599\n",
            "\n",
            "File dev-v1.1.json successfully loaded\n",
            "Dev data has 10570 examples total\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  176\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  0\n",
            "Processed 10394 examples of total 10570\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\temp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "\n",
            "Preprocessing train:   0%|          | 0/442 [00:00<?, ?it/s]\n",
            "Preprocessing train:   0%|          | 2/442 [00:00<00:35, 12.24it/s]\n",
            "Preprocessing train:   1%|▏         | 6/442 [00:00<00:23, 18.25it/s]\n",
            "Preprocessing train:   2%|▏         | 8/442 [00:00<00:23, 18.52it/s]\n",
            "Preprocessing train:   2%|▏         | 11/442 [00:00<00:21, 20.52it/s]\n",
            "Preprocessing train:   3%|▎         | 14/442 [00:00<00:24, 17.74it/s]\n",
            "Preprocessing train:   4%|▍         | 17/442 [00:00<00:22, 18.73it/s]\n",
            "Preprocessing train:   4%|▍         | 19/442 [00:01<00:26, 16.14it/s]\n",
            "Preprocessing train:   5%|▍         | 21/442 [00:01<00:27, 15.30it/s]\n",
            "Preprocessing train:   5%|▌         | 23/442 [00:01<00:26, 15.59it/s]\n",
            "Preprocessing train:   6%|▌         | 26/442 [00:01<00:22, 18.60it/s]\n",
            "Preprocessing train:   7%|▋         | 30/442 [00:01<00:17, 23.87it/s]\n",
            "Preprocessing train:   8%|▊         | 36/442 [00:01<00:12, 33.15it/s]\n",
            "Preprocessing train:   9%|▉         | 41/442 [00:01<00:10, 37.66it/s]\n",
            "Preprocessing train:  10%|█         | 46/442 [00:01<00:11, 35.17it/s]\n",
            "Preprocessing train:  11%|█▏        | 50/442 [00:02<00:11, 33.23it/s]\n",
            "Preprocessing train:  13%|█▎        | 56/442 [00:02<00:10, 36.13it/s]\n",
            "Preprocessing train:  14%|█▎        | 60/442 [00:02<00:11, 33.91it/s]\n",
            "Preprocessing train:  14%|█▍        | 64/442 [00:02<00:11, 33.77it/s]\n",
            "Preprocessing train:  16%|█▌        | 69/442 [00:02<00:09, 37.34it/s]\n",
            "Preprocessing train:  17%|█▋        | 74/442 [00:02<00:09, 38.13it/s]\n",
            "Preprocessing train:  18%|█▊        | 78/442 [00:02<00:10, 35.85it/s]\n",
            "Preprocessing train:  19%|█▉        | 83/442 [00:02<00:09, 38.44it/s]\n",
            "Preprocessing train:  20%|█▉        | 87/442 [00:03<00:11, 30.80it/s]\n",
            "Preprocessing train:  21%|██        | 91/442 [00:03<00:11, 29.54it/s]\n",
            "Preprocessing train:  21%|██▏       | 95/442 [00:03<00:12, 28.09it/s]\n",
            "Preprocessing train:  22%|██▏       | 98/442 [00:03<00:12, 27.44it/s]\n",
            "Preprocessing train:  23%|██▎       | 102/442 [00:03<00:12, 28.24it/s]\n",
            "Preprocessing train:  24%|██▍       | 105/442 [00:03<00:13, 25.08it/s]\n",
            "Preprocessing train:  24%|██▍       | 108/442 [00:03<00:13, 25.63it/s]\n",
            "Preprocessing train:  26%|██▌       | 113/442 [00:04<00:11, 29.40it/s]\n",
            "Preprocessing train:  26%|██▌       | 116/442 [00:04<00:11, 28.82it/s]\n",
            "Preprocessing train:  27%|██▋       | 121/442 [00:04<00:09, 32.54it/s]\n",
            "Preprocessing train:  29%|██▉       | 128/442 [00:04<00:08, 38.92it/s]\n",
            "Preprocessing train:  30%|███       | 133/442 [00:04<00:07, 41.37it/s]\n",
            "Preprocessing train:  31%|███       | 138/442 [00:04<00:07, 38.19it/s]\n",
            "Preprocessing train:  33%|███▎      | 144/442 [00:04<00:06, 43.55it/s]\n",
            "Preprocessing train:  34%|███▎      | 149/442 [00:04<00:06, 42.85it/s]\n",
            "Preprocessing train:  35%|███▍      | 154/442 [00:05<00:08, 35.46it/s]\n",
            "Preprocessing train:  36%|███▌      | 158/442 [00:05<00:09, 28.81it/s]\n",
            "Preprocessing train:  37%|███▋      | 162/442 [00:05<00:09, 29.03it/s]\n",
            "Preprocessing train:  38%|███▊      | 166/442 [00:05<00:09, 28.46it/s]\n",
            "Preprocessing train:  38%|███▊      | 170/442 [00:05<00:09, 27.87it/s]\n",
            "Preprocessing train:  39%|███▉      | 173/442 [00:05<00:10, 26.74it/s]\n",
            "Preprocessing train:  40%|███▉      | 176/442 [00:06<00:10, 26.21it/s]\n",
            "Preprocessing train:  40%|████      | 179/442 [00:06<00:10, 25.93it/s]\n",
            "Preprocessing train:  41%|████      | 182/442 [00:06<00:10, 25.97it/s]\n",
            "Preprocessing train:  42%|████▏     | 185/442 [00:06<00:11, 22.44it/s]\n",
            "Preprocessing train:  43%|████▎     | 188/442 [00:06<00:12, 19.87it/s]\n",
            "Preprocessing train:  43%|████▎     | 191/442 [00:06<00:11, 21.28it/s]\n",
            "Preprocessing train:  44%|████▍     | 194/442 [00:06<00:10, 23.07it/s]\n",
            "Preprocessing train:  45%|████▍     | 198/442 [00:07<00:09, 24.59it/s]\n",
            "Preprocessing train:  45%|████▌     | 201/442 [00:07<00:09, 25.21it/s]\n",
            "Preprocessing train:  46%|████▌     | 204/442 [00:07<00:09, 24.75it/s]\n",
            "Preprocessing train:  47%|████▋     | 207/442 [00:07<00:09, 25.48it/s]\n",
            "Preprocessing train:  48%|████▊     | 210/442 [00:07<00:11, 20.90it/s]\n",
            "Preprocessing train:  48%|████▊     | 213/442 [00:07<00:13, 17.41it/s]\n",
            "Preprocessing train:  49%|████▉     | 216/442 [00:07<00:12, 18.21it/s]\n",
            "Preprocessing train:  49%|████▉     | 218/442 [00:08<00:13, 16.62it/s]\n",
            "Preprocessing train:  50%|█████     | 221/442 [00:08<00:12, 18.14it/s]\n",
            "Preprocessing train:  50%|█████     | 223/442 [00:08<00:14, 15.64it/s]\n",
            "Preprocessing train:  51%|█████     | 225/442 [00:08<00:14, 14.72it/s]\n",
            "Preprocessing train:  51%|█████▏    | 227/442 [00:08<00:15, 14.07it/s]\n",
            "Preprocessing train:  52%|█████▏    | 229/442 [00:08<00:14, 14.76it/s]\n",
            "Preprocessing train:  52%|█████▏    | 231/442 [00:08<00:13, 15.51it/s]\n",
            "Preprocessing train:  53%|█████▎    | 233/442 [00:09<00:13, 15.41it/s]\n",
            "Preprocessing train:  53%|█████▎    | 235/442 [00:09<00:13, 15.09it/s]\n",
            "Preprocessing train:  54%|█████▎    | 237/442 [00:09<00:12, 16.14it/s]\n",
            "Preprocessing train:  54%|█████▍    | 239/442 [00:09<00:12, 16.24it/s]\n",
            "Preprocessing train:  55%|█████▍    | 242/442 [00:09<00:10, 18.47it/s]\n",
            "Preprocessing train:  55%|█████▌    | 244/442 [00:09<00:10, 18.03it/s]\n",
            "Preprocessing train:  56%|█████▌    | 247/442 [00:09<00:09, 20.15it/s]\n",
            "Preprocessing train:  57%|█████▋    | 250/442 [00:10<00:10, 18.73it/s]\n",
            "Preprocessing train:  57%|█████▋    | 253/442 [00:10<00:10, 17.38it/s]\n",
            "Preprocessing train:  58%|█████▊    | 256/442 [00:10<00:10, 17.41it/s]\n",
            "Preprocessing train:  58%|█████▊    | 258/442 [00:10<00:11, 16.20it/s]\n",
            "Preprocessing train:  59%|█████▉    | 260/442 [00:10<00:11, 16.07it/s]\n",
            "Preprocessing train:  59%|█████▉    | 262/442 [00:10<00:11, 15.99it/s]\n",
            "Preprocessing train:  60%|█████▉    | 264/442 [00:10<00:11, 14.96it/s]\n",
            "Preprocessing train:  60%|██████    | 267/442 [00:11<00:10, 17.34it/s]\n",
            "Preprocessing train:  61%|██████    | 270/442 [00:11<00:09, 18.01it/s]\n",
            "Preprocessing train:  62%|██████▏   | 273/442 [00:11<00:08, 19.53it/s]\n",
            "Preprocessing train:  62%|██████▏   | 275/442 [00:11<00:10, 16.47it/s]\n",
            "Preprocessing train:  63%|██████▎   | 277/442 [00:11<00:10, 15.87it/s]\n",
            "Preprocessing train:  63%|██████▎   | 279/442 [00:11<00:10, 15.27it/s]\n",
            "Preprocessing train:  64%|██████▎   | 281/442 [00:11<00:10, 15.91it/s]\n",
            "Preprocessing train:  64%|██████▍   | 283/442 [00:12<00:11, 13.87it/s]\n",
            "Preprocessing train:  65%|██████▍   | 286/442 [00:12<00:09, 17.09it/s]\n",
            "Preprocessing train:  65%|██████▌   | 288/442 [00:12<00:08, 17.56it/s]\n",
            "Preprocessing train:  66%|██████▌   | 290/442 [00:12<00:08, 17.91it/s]\n",
            "Preprocessing train:  66%|██████▌   | 292/442 [00:12<00:08, 17.52it/s]\n",
            "Preprocessing train:  67%|██████▋   | 294/442 [00:12<00:08, 17.32it/s]\n",
            "Preprocessing train:  67%|██████▋   | 297/442 [00:12<00:08, 16.58it/s]\n",
            "Preprocessing train:  68%|██████▊   | 300/442 [00:13<00:07, 18.92it/s]\n",
            "Preprocessing train:  69%|██████▊   | 303/442 [00:13<00:07, 19.54it/s]\n",
            "Preprocessing train:  69%|██████▉   | 305/442 [00:13<00:09, 15.22it/s]\n",
            "Preprocessing train:  69%|██████▉   | 307/442 [00:13<00:09, 14.01it/s]\n",
            "Preprocessing train:  70%|███████   | 310/442 [00:13<00:08, 16.30it/s]\n",
            "Preprocessing train:  71%|███████   | 312/442 [00:13<00:08, 15.96it/s]\n",
            "Preprocessing train:  71%|███████▏  | 315/442 [00:13<00:06, 18.77it/s]\n",
            "Preprocessing train:  72%|███████▏  | 318/442 [00:14<00:06, 18.21it/s]\n",
            "Preprocessing train:  73%|███████▎  | 321/442 [00:14<00:06, 17.97it/s]\n",
            "Preprocessing train:  73%|███████▎  | 323/442 [00:14<00:06, 18.29it/s]\n",
            "Preprocessing train:  74%|███████▎  | 325/442 [00:14<00:06, 16.72it/s]\n",
            "Preprocessing train:  74%|███████▍  | 327/442 [00:14<00:08, 13.47it/s]\n",
            "Preprocessing train:  74%|███████▍  | 329/442 [00:14<00:09, 11.97it/s]\n",
            "Preprocessing train:  75%|███████▌  | 332/442 [00:15<00:07, 14.59it/s]\n",
            "Preprocessing train:  76%|███████▌  | 334/442 [00:15<00:07, 14.99it/s]\n",
            "Preprocessing train:  76%|███████▌  | 337/442 [00:15<00:06, 16.88it/s]\n",
            "Preprocessing train:  77%|███████▋  | 340/442 [00:15<00:05, 18.92it/s]\n",
            "Preprocessing train:  78%|███████▊  | 343/442 [00:15<00:04, 21.35it/s]\n",
            "Preprocessing train:  78%|███████▊  | 346/442 [00:15<00:04, 19.75it/s]\n",
            "Preprocessing train:  79%|███████▉  | 349/442 [00:15<00:04, 19.25it/s]\n",
            "Preprocessing train:  80%|███████▉  | 352/442 [00:16<00:05, 17.43it/s]\n",
            "Preprocessing train:  80%|████████  | 355/442 [00:16<00:04, 18.51it/s]\n",
            "Preprocessing train:  81%|████████  | 357/442 [00:16<00:04, 17.71it/s]\n",
            "Preprocessing train:  81%|████████▏ | 360/442 [00:16<00:04, 17.15it/s]\n",
            "Preprocessing train:  82%|████████▏ | 362/442 [00:16<00:05, 14.76it/s]\n",
            "Preprocessing train:  82%|████████▏ | 364/442 [00:16<00:05, 13.95it/s]\n",
            "Preprocessing train:  83%|████████▎ | 366/442 [00:17<00:05, 14.24it/s]\n",
            "Preprocessing train:  83%|████████▎ | 369/442 [00:17<00:04, 15.77it/s]\n",
            "Preprocessing train:  84%|████████▍ | 371/442 [00:17<00:05, 13.21it/s]\n",
            "Preprocessing train:  85%|████████▍ | 374/442 [00:17<00:04, 13.81it/s]\n",
            "Preprocessing train:  85%|████████▌ | 376/442 [00:17<00:04, 13.55it/s]\n",
            "Preprocessing train:  86%|████████▌ | 379/442 [00:17<00:03, 16.71it/s]\n",
            "Preprocessing train:  86%|████████▌ | 381/442 [00:18<00:04, 15.10it/s]\n",
            "Preprocessing train:  87%|████████▋ | 383/442 [00:18<00:04, 14.42it/s]\n",
            "Preprocessing train:  87%|████████▋ | 385/442 [00:18<00:03, 14.82it/s]\n",
            "Preprocessing train:  88%|████████▊ | 388/442 [00:18<00:03, 17.82it/s]\n",
            "Preprocessing train:  88%|████████▊ | 391/442 [00:18<00:02, 17.21it/s]\n",
            "Preprocessing train:  89%|████████▉ | 393/442 [00:18<00:03, 15.98it/s]\n",
            "Preprocessing train:  89%|████████▉ | 395/442 [00:18<00:03, 15.63it/s]\n",
            "Preprocessing train:  90%|████████▉ | 397/442 [00:19<00:03, 12.13it/s]\n",
            "Preprocessing train:  90%|█████████ | 399/442 [00:19<00:03, 11.96it/s]\n",
            "Preprocessing train:  91%|█████████ | 401/442 [00:19<00:03, 12.91it/s]\n",
            "Preprocessing train:  91%|█████████ | 403/442 [00:19<00:02, 14.28it/s]\n",
            "Preprocessing train:  92%|█████████▏| 406/442 [00:19<00:02, 16.72it/s]\n",
            "Preprocessing train:  93%|█████████▎| 409/442 [00:19<00:01, 19.31it/s]\n",
            "Preprocessing train:  93%|█████████▎| 412/442 [00:20<00:01, 19.65it/s]\n",
            "Preprocessing train:  94%|█████████▍| 415/442 [00:20<00:01, 20.86it/s]\n",
            "Preprocessing train:  95%|█████████▍| 418/442 [00:20<00:01, 19.57it/s]\n",
            "Preprocessing train:  95%|█████████▌| 421/442 [00:20<00:01, 14.73it/s]\n",
            "Preprocessing train:  96%|█████████▌| 423/442 [00:20<00:01, 13.72it/s]\n",
            "Preprocessing train:  96%|█████████▌| 425/442 [00:20<00:01, 13.00it/s]\n",
            "Preprocessing train:  97%|█████████▋| 427/442 [00:21<00:01, 14.01it/s]\n",
            "Preprocessing train:  97%|█████████▋| 429/442 [00:21<00:00, 13.89it/s]\n",
            "Preprocessing train:  98%|█████████▊| 432/442 [00:21<00:00, 16.58it/s]\n",
            "Preprocessing train:  98%|█████████▊| 434/442 [00:21<00:00, 13.94it/s]\n",
            "Preprocessing train:  99%|█████████▊| 436/442 [00:21<00:00, 14.36it/s]\n",
            "Preprocessing train:  99%|█████████▉| 439/442 [00:21<00:00, 16.65it/s]\n",
            "Preprocessing train: 100%|█████████▉| 441/442 [00:21<00:00, 16.59it/s]\n",
            "Preprocessing train: 100%|██████████| 442/442 [00:22<00:00, 20.06it/s]\n",
            "\n",
            "Preprocessing dev:   0%|          | 0/48 [00:00<?, ?it/s]\n",
            "Preprocessing dev:   2%|▏         | 1/48 [00:00<00:06,  7.58it/s]\n",
            "Preprocessing dev:   6%|▋         | 3/48 [00:00<00:03, 12.12it/s]\n",
            "Preprocessing dev:  10%|█         | 5/48 [00:00<00:03, 11.62it/s]\n",
            "Preprocessing dev:  15%|█▍        | 7/48 [00:00<00:03, 10.41it/s]\n",
            "Preprocessing dev:  21%|██        | 10/48 [00:00<00:02, 14.49it/s]\n",
            "Preprocessing dev:  25%|██▌       | 12/48 [00:00<00:02, 14.11it/s]\n",
            "Preprocessing dev:  29%|██▉       | 14/48 [00:01<00:02, 15.23it/s]\n",
            "Preprocessing dev:  33%|███▎      | 16/48 [00:01<00:02, 12.83it/s]\n",
            "Preprocessing dev:  42%|████▏     | 20/48 [00:01<00:01, 18.11it/s]\n",
            "Preprocessing dev:  50%|█████     | 24/48 [00:01<00:01, 20.98it/s]\n",
            "Preprocessing dev:  56%|█████▋    | 27/48 [00:01<00:01, 20.13it/s]\n",
            "Preprocessing dev:  62%|██████▎   | 30/48 [00:01<00:00, 19.67it/s]\n",
            "Preprocessing dev:  69%|██████▉   | 33/48 [00:01<00:00, 21.18it/s]\n",
            "Preprocessing dev:  75%|███████▌  | 36/48 [00:02<00:00, 17.63it/s]\n",
            "Preprocessing dev:  79%|███████▉  | 38/48 [00:02<00:00, 16.09it/s]\n",
            "Preprocessing dev:  83%|████████▎ | 40/48 [00:02<00:00, 16.15it/s]\n",
            "Preprocessing dev:  88%|████████▊ | 42/48 [00:02<00:00, 16.27it/s]\n",
            "Preprocessing dev:  92%|█████████▏| 44/48 [00:02<00:00, 16.66it/s]\n",
            "Preprocessing dev:  96%|█████████▌| 46/48 [00:02<00:00, 15.90it/s]\n",
            "Preprocessing dev: 100%|██████████| 48/48 [00:02<00:00, 15.56it/s]\n",
            "Preprocessing dev: 100%|██████████| 48/48 [00:02<00:00, 16.10it/s]\n"
          ]
        }
      ],
      "source": [
        "# Used for obtaining the training data\n",
        "! python ./preprocessing/download_wordvecs.py --download_dir ./data\n",
        "! python ./preprocessing/squad_preprocess.py --data_dir ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe88d2",
      "metadata": {},
      "source": [
        "From the paper:\n",
        "\"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent\n",
        "units, maxout layers, and linear layers. All LSTMs have randomly initialized parameters and an\n",
        "initial state of zero. Sentinel vectors are randomly initialized and optimized during training. For\n",
        "the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool size of\n",
        "16. We use dropout to regularize our network during training (Srivastava et al., 2014), and optimize\n",
        "the model using ADAM (Kingma & Ba, 2014). All models are implemented and trained with\n",
        "Chainer (Tokui et al., 2015).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6aba24a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\_environment_check.py:72: UserWarning: \n",
            "--------------------------------------------------------------------------------\n",
            "CuPy (cupy) version 12.3.0 may not be compatible with this version of Chainer.\n",
            "Please consider installing the supported version by running:\n",
            "  $ pip install 'cupy>=7.7.0,<8.0.0'\n",
            "\n",
            "See the following page for more details:\n",
            "  https://docs.cupy.dev/en/latest/install.html\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  warnings.warn(msg.format(\n"
          ]
        }
      ],
      "source": [
        "from model import DynamicCoattentionNW\n",
        "import chainer as ch\n",
        "\n",
        "\n",
        "max_seq_length = 600\n",
        "\n",
        "hid_state_size = 200\n",
        "\n",
        "dyn_dec_max_it = 4\n",
        "maxout_pool_size = 16\n",
        "\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1937fde7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GLoVE vectors from file: ./data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2196017/2196017 [02:03<00:00, 17782.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2196018, 300)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from preprocessing.vocab import get_glove\n",
        "\n",
        "glove_path = \"./data/glove.840B.300d.txt\"\n",
        "glove_vector_size = 300\n",
        "emb_mat, word2id, id2word = get_glove(glove_path, glove_vector_size)\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "print(emb_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "7e5a42c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Refilling batches...\n",
            "Refilling batches took 0.04088926315307617 seconds\n",
            "(2, 600, 600)\n",
            "(2, 600, 400)\n",
            "[526 143]\n",
            "[162 443]\n",
            "[[ 95 100]\n",
            " [ 63  64]]\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "\n",
        "%autoreload 2\n",
        "\n",
        "from data_batcher import get_batch_generator\n",
        "batchsize = 2\n",
        "# batchsize = 128\n",
        "\n",
        "train_context_path = \"./data/train.context\"\n",
        "train_qn_path = \"./data/train.question\"\n",
        "train_ans_path = \"./data/train.answer\"\n",
        "train_span_path = \"./data/train.span\"\n",
        "\n",
        "batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "#temp\n",
        "from model import DocAndQuesEncoder, CoattentionEncoder, DynamicPointingDecoder, DynamicCoattentionNW\n",
        "# enc = DocAndQuesEncoder(emb_mat, dropout, hid_state_size)\n",
        "# enc2 = CoattentionEncoder(dropout, hid_state_size)\n",
        "# dec = DynamicPointingDecoder(dropout, hid_state_size, maxout_pool_size, dyn_dec_max_it)\n",
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "for batch in batch_gen:\n",
        "    s, e = model.forward(batch.context_ids, batch.qn_ids)\n",
        "    # D, Q = enc.forward(batch.context_ids, batch.qn_ids)\n",
        "    # U = enc2.forward(D, Q)\n",
        "    # s, e = dec.forward(U)\n",
        "    print(s)\n",
        "    print(e)\n",
        "\n",
        "\n",
        "    print(batch.ans_span)\n",
        "    # print(batch.ans_tokens)   \n",
        "    break\n",
        "\n",
        "# train_iter = ch.iterators.SerialIterator(train, batchsize)\n",
        "# test_iter = ch.iterators.SerialIterator(test, batchsize, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf5301c",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88519d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam()\n",
        "optimizer.setup(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea519ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the updater, using the optimizer\n",
        "updater = ch.training.StandardUpdater(train_iter, optimizer, device=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50999c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up a trainer\n",
        "max_epoch = 10\n",
        "trainer = ch.training.Trainer(updater, (max_epoch, 'epoch'), out='result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cd42e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "#see https://docs.chainer.org/en/stable/guides/serializers.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLNLP-Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
