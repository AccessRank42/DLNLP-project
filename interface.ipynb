{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f8df10b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Used for obtaining the training data\n",
        "# ! python ./preprocessing/download_wordvecs.py --download_dir ./data\n",
        "# ! python ./preprocessing/squad_preprocess.py --data_dir ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe88d2",
      "metadata": {},
      "source": [
        "From the paper:\n",
        "\"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent\n",
        "units, maxout layers, and linear layers. All LSTMs have randomly initialized parameters and an\n",
        "initial state of zero. Sentinel vectors are randomly initialized and optimized during training. For\n",
        "the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool size of\n",
        "16. We use dropout to regularize our network during training (Srivastava et al., 2014), and optimize\n",
        "the model using ADAM (Kingma & Ba, 2014). All models are implemented and trained with\n",
        "Chainer (Tokui et al., 2015).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6aba24a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\_environment_check.py:72: UserWarning: \n",
            "--------------------------------------------------------------------------------\n",
            "CuPy (cupy) version 12.3.0 may not be compatible with this version of Chainer.\n",
            "Please consider installing the supported version by running:\n",
            "  $ pip install 'cupy>=7.7.0,<8.0.0'\n",
            "\n",
            "See the following page for more details:\n",
            "  https://docs.cupy.dev/en/latest/install.html\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  warnings.warn(msg.format(\n"
          ]
        }
      ],
      "source": [
        "from model import DynamicCoattentionNW\n",
        "import chainer as ch\n",
        "import numpy as np\n",
        "import chainer.functions as F\n",
        "\n",
        "\n",
        "max_seq_length = 600\n",
        "\n",
        "hid_state_size = 200\n",
        "\n",
        "dyn_dec_max_it = 4\n",
        "maxout_pool_size = 16\n",
        "\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1937fde7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GLoVE vectors from file: ./data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 1902150/2196017 [01:35<00:15, 19142.98it/s]"
          ]
        }
      ],
      "source": [
        "from preprocessing.vocab import get_glove\n",
        "\n",
        "glove_path = \"./data/glove.840B.300d.txt\"\n",
        "glove_vector_size = 300\n",
        "emb_mat, word2id, id2word = get_glove(glove_path, glove_vector_size)\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "print(emb_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88519d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam()\n",
        "optimizer.setup(model)\n",
        "\n",
        "model.to_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcac50e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_batcher import get_batch_generator\n",
        "from chainer.backends import cuda\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# batchsize = 2 # for quick test only\n",
        "batchsize = 32 # seems to be the fastest overall\n",
        "# batchsize = 64\n",
        "max_epoch = 10\n",
        "epochs_pre_trained = 0\n",
        "show_mean_loss_at_batches = 200\n",
        "\n",
        "train_file_lines = 86307\n",
        "\n",
        "train_context_path = \"./data/train.context\"\n",
        "train_qn_path = \"./data/train.question\"\n",
        "train_ans_path = \"./data/train.answer\"\n",
        "train_span_path = \"./data/train.span\"\n",
        "\n",
        "# try loading a trained model\n",
        "for i in range(max_epoch, 0, -1):\n",
        "    try:\n",
        "        ch.serializers.load_npz('DCANW_E{}.model'.format(i), model)\n",
        "        print(\"Model DCANW_E{}.model loaded successfully\".format(i))\n",
        "        epochs_pre_trained = i\n",
        "        break\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "\n",
        "\n",
        "for epoch in range(1+epochs_pre_trained, max_epoch+1):\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "    batch_id = 0\n",
        "    losses = []\n",
        "    for batch in tqdm(batch_gen, total=train_file_lines/batchsize): # progress bar\n",
        "        ch.cuda.cupy.get_default_pinned_memory_pool().free_all_blocks() #free up memory\n",
        "        batch_id += 1\n",
        "        model.reset_state()\n",
        "    \n",
        "        # Calculate the prediction & loss of the network\n",
        "        c_seq = ch.Variable(cuda.to_gpu(batch.context_ids))\n",
        "        q_seq = ch.Variable(cuda.to_gpu(batch.qn_ids))\n",
        "        ground_truth = cuda.to_gpu(batch.ans_span)\n",
        "        s_prediction, e_prediction, loss = model(c_seq, q_seq, ground_truth)\n",
        "\n",
        "        # Calculate the gradients in the network\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update all the trainable parameters\n",
        "        optimizer.update()\n",
        "\n",
        "        # print(\"Batch done\")\n",
        "        losses.append(loss.item())\n",
        "        if batch_id % show_mean_loss_at_batches == 0:\n",
        "            print(\"Current mean loss of epoch: {}\".format(np.mean(losses)))\n",
        "\n",
        "    print(\"Mean loss of epoch {}: {}\".format(epoch, np.mean(losses)))\n",
        "    ch.serializers.save_npz('DCANW_E{}.model'.format(epoch), model)\n",
        "    \n",
        "\n",
        "\n",
        "print(\"Training for {} epochs finished\".format(max_epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a51e162",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    #  # Display the training loss\n",
        "    #     print('epoch:{} train_loss:{:.04f} '.format(\n",
        "    #         epoch, ))\n",
        "\n",
        "    #     test_losses = []\n",
        "    #     test_accuracies = []\n",
        "    #     for test_batch in test_iter:\n",
        "    #         image_test, target_test = concat_examples(test_batch, gpu_id)\n",
        "\n",
        "    #         # Forward the test data\n",
        "    #         prediction_test = model(image_test)\n",
        "\n",
        "    #         # Calculate the loss\n",
        "    #         loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
        "    #         test_losses.append(to_cpu(loss_test.array))\n",
        "\n",
        "    #         # Calculate the accuracy\n",
        "    #         accuracy = F.accuracy(prediction_test, target_test)\n",
        "    #         accuracy.to_cpu()\n",
        "    #         test_accuracies.append(accuracy.array)\n",
        "\n",
        "    #     test_iter.reset()\n",
        "\n",
        "    #     print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
        "    #         np.mean(test_losses), np.mean(test_accuracies)))\n",
        "\n",
        "# # train_iter = ch.iterators.SerialIterator(train, batchsize)\n",
        "# # test_iter = ch.iterators.SerialIterator(test, batchsize, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea519ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create the updater, using the optimizer --> not used\n",
        "# updater = ch.training.StandardUpdater(train_iter, optimizer, device=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50999c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set up a trainer -- not used\n",
        "# max_epoch = 10\n",
        "# trainer = ch.training.Trainer(updater, (max_epoch, 'epoch'), out='result')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLNLP-Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
