{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8df10b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Used for obtaining the training data\n",
        "# ! python ./preprocessing/download_wordvecs.py --download_dir ./data\n",
        "# ! python ./preprocessing/squad_preprocess.py --data_dir ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe88d2",
      "metadata": {},
      "source": [
        "From the paper:\n",
        "\"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent\n",
        "units, maxout layers, and linear layers. All LSTMs have randomly initialized parameters and an\n",
        "initial state of zero. Sentinel vectors are randomly initialized and optimized during training. For\n",
        "the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool size of\n",
        "16. We use dropout to regularize our network during training (Srivastava et al., 2014), and optimize\n",
        "the model using ADAM (Kingma & Ba, 2014). All models are implemented and trained with\n",
        "Chainer (Tokui et al., 2015).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aba24a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import DynamicCoattentionNW\n",
        "import chainer as ch\n",
        "import chainer.functions as F\n",
        "\n",
        "\n",
        "max_seq_length = 600\n",
        "\n",
        "hid_state_size = 200\n",
        "\n",
        "dyn_dec_max_it = 4\n",
        "maxout_pool_size = 16\n",
        "\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1937fde7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from preprocessing.vocab import get_glove\n",
        "\n",
        "glove_path = \"./data/glove.840B.300d.txt\"\n",
        "glove_vector_size = 300\n",
        "emb_mat, word2id, id2word = get_glove(glove_path, glove_vector_size)\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "print(emb_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5a42c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "\n",
        "# %autoreload 2\n",
        "\n",
        "# from data_batcher import get_batch_generator\n",
        "# batchsize = 4\n",
        "# # batchsize = 128\n",
        "\n",
        "# train_context_path = \"./data/train.context\"\n",
        "# train_qn_path = \"./data/train.question\"\n",
        "# train_ans_path = \"./data/train.answer\"\n",
        "# train_span_path = \"./data/train.span\"\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "# #temp\n",
        "# from model import DocAndQuesEncoder, CoattentionEncoder, DynamicPointingDecoder, DynamicCoattentionNW\n",
        "# # enc = DocAndQuesEncoder(emb_mat, dropout, hid_state_size)\n",
        "# # enc2 = CoattentionEncoder(dropout, hid_state_size)\n",
        "# # dec = DynamicPointingDecoder(dropout, hid_state_size, maxout_pool_size, dyn_dec_max_it)\n",
        "# model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# for batch in batch_gen:\n",
        "#     s, e = model.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # D, Q = enc.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # U = enc2.forward(D, Q)\n",
        "#     # s, e = dec.forward(U)\n",
        "#     print(s)\n",
        "#     print(e)\n",
        "\n",
        "\n",
        "#     print(batch.ans_span)\n",
        "#     # print(batch.ans_tokens)   \n",
        "#     break\n",
        "\n",
        "# # train_iter = ch.iterators.SerialIterator(train, batchsize)\n",
        "# # test_iter = ch.iterators.SerialIterator(test, batchsize, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afff63e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def a_loss_function(prediction, ground_truth):\n",
        "    s_p, e_p = prediction\n",
        "    s_t = ground_truth[:,0].astype('f')\n",
        "    e_t = ground_truth[:,1].astype('f')\n",
        "\n",
        "    start_loss = F.mean_squared_error(s_p.astype('f'), s_t)\n",
        "    end_loss = F.mean_squared_error(e_p.astype('f'), e_t)\n",
        "    total_loss = start_loss + end_loss\n",
        "\n",
        "    if (s_p>e_p).any(): #penalize impossible start > end predictions\n",
        "        total_loss *= 2\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88519d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam()\n",
        "optimizer.setup(model)\n",
        "\n",
        "model.to_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcac50e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_batcher import get_batch_generator\n",
        "from chainer.backends import cuda\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# batchsize = 16\n",
        "# batchsize = 32\n",
        "batchsize = 64\n",
        "max_epoch = 10\n",
        "epochs_pre_trained = 0\n",
        "\n",
        "train_file_lines = 86307\n",
        "\n",
        "train_context_path = \"./data/train.context\"\n",
        "train_qn_path = \"./data/train.question\"\n",
        "train_ans_path = \"./data/train.answer\"\n",
        "train_span_path = \"./data/train.span\"\n",
        "\n",
        "for i in range(max_epoch, 0, -1):\n",
        "    try:\n",
        "        ch.serializers.load_npz('DCANW_E{}.model'.format(i), model)\n",
        "        print(\"Model DCANW_E{}.model loaded successfully\".format(i))\n",
        "        epochs_pre_trained = i\n",
        "        break\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "for epoch in range(1+epochs_pre_trained, max_epoch+1):\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "    batch_id = 0\n",
        "    for batch in tqdm(batch_gen, total=train_file_lines/batchsize):\n",
        "        batch_id += 1\n",
        "    # for batch in batch_gen:\n",
        "        # tic = time.time()\n",
        "        # Calculate the prediction of the network\n",
        "        c_seq = ch.Variable(cuda.to_gpu(batch.context_ids))\n",
        "        q_seq = ch.Variable(cuda.to_gpu(batch.qn_ids))\n",
        "        prediction = model(c_seq, q_seq)\n",
        "        # prediction = model(batch.context_ids, batch.qn_ids)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = a_loss_function(prediction, cuda.to_gpu(batch.ans_span))\n",
        "\n",
        "        # Calculate the gradients in the network\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update all the trainable parameters\n",
        "        optimizer.update()\n",
        "\n",
        "        # print(\"Batch done\")\n",
        "        # toc = time.time()\n",
        "        # print(\"Current batch took {} seconds\".format(toc-tic))\n",
        "        if batch_id % 100 == 0:\n",
        "            print(loss.item())\n",
        "\n",
        "    ch.serializers.save_npz('DCANW_E{}.model'.format(epoch), model)\n",
        "    \n",
        "\n",
        "    #  # Display the training loss\n",
        "    #     print('epoch:{} train_loss:{:.04f} '.format(\n",
        "    #         epoch, ))\n",
        "\n",
        "    #     test_losses = []\n",
        "    #     test_accuracies = []\n",
        "    #     for test_batch in test_iter:\n",
        "    #         image_test, target_test = concat_examples(test_batch, gpu_id)\n",
        "\n",
        "    #         # Forward the test data\n",
        "    #         prediction_test = model(image_test)\n",
        "\n",
        "    #         # Calculate the loss\n",
        "    #         loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
        "    #         test_losses.append(to_cpu(loss_test.array))\n",
        "\n",
        "    #         # Calculate the accuracy\n",
        "    #         accuracy = F.accuracy(prediction_test, target_test)\n",
        "    #         accuracy.to_cpu()\n",
        "    #         test_accuracies.append(accuracy.array)\n",
        "\n",
        "    #     test_iter.reset()\n",
        "\n",
        "    #     print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
        "    #         np.mean(test_losses), np.mean(test_accuracies)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea519ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create the updater, using the optimizer\n",
        "# updater = ch.training.StandardUpdater(train_iter, optimizer, device=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50999c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set up a trainer\n",
        "# max_epoch = 10\n",
        "# trainer = ch.training.Trainer(updater, (max_epoch, 'epoch'), out='result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cd42e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "#see https://docs.chainer.org/en/stable/guides/serializers.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLNLP-Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
