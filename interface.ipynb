{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f8df10b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading wordvecs to ./data\n",
            "File glove.840B.300d.zip successfully loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\temp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will download SQuAD datasets to ./data\n",
            "Will put preprocessed SQuAD datasets in ./data\n",
            "File train-v1.1.json successfully loaded\n",
            "Train data has 87599 examples total\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  97\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1173\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  23\n",
            "Processed 86306 examples of total 87599\n",
            "\n",
            "File dev-v1.1.json successfully loaded\n",
            "Dev data has 10570 examples total\n",
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  176\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  0\n",
            "Processed 10394 examples of total 10570\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\temp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "\n",
            "Preprocessing train:   0%|          | 0/442 [00:00<?, ?it/s]\n",
            "Preprocessing train:   0%|          | 2/442 [00:00<00:36, 11.95it/s]\n",
            "Preprocessing train:   1%|▏         | 6/442 [00:00<00:25, 17.36it/s]\n",
            "Preprocessing train:   2%|▏         | 9/442 [00:00<00:21, 19.78it/s]\n",
            "Preprocessing train:   3%|▎         | 12/442 [00:00<00:25, 16.82it/s]\n",
            "Preprocessing train:   3%|▎         | 15/442 [00:00<00:21, 19.66it/s]\n",
            "Preprocessing train:   4%|▍         | 18/442 [00:01<00:24, 17.48it/s]\n",
            "Preprocessing train:   5%|▍         | 20/442 [00:01<00:27, 15.09it/s]\n",
            "Preprocessing train:   5%|▍         | 22/442 [00:01<00:26, 15.67it/s]\n",
            "Preprocessing train:   5%|▌         | 24/442 [00:01<00:26, 16.03it/s]\n",
            "Preprocessing train:   7%|▋         | 30/442 [00:01<00:16, 24.85it/s]\n",
            "Preprocessing train:   8%|▊         | 35/442 [00:01<00:13, 30.78it/s]\n",
            "Preprocessing train:  10%|▉         | 42/442 [00:01<00:10, 39.12it/s]\n",
            "Preprocessing train:  11%|█         | 47/442 [00:01<00:10, 36.01it/s]\n",
            "Preprocessing train:  12%|█▏        | 51/442 [00:02<00:11, 34.42it/s]\n",
            "Preprocessing train:  13%|█▎        | 56/442 [00:02<00:10, 36.25it/s]\n",
            "Preprocessing train:  14%|█▎        | 60/442 [00:02<00:11, 34.26it/s]\n",
            "Preprocessing train:  14%|█▍        | 64/442 [00:02<00:11, 34.05it/s]\n",
            "Preprocessing train:  16%|█▌        | 70/442 [00:02<00:09, 39.40it/s]\n",
            "Preprocessing train:  17%|█▋        | 75/442 [00:02<00:10, 36.67it/s]\n",
            "Preprocessing train:  18%|█▊        | 80/442 [00:02<00:09, 39.05it/s]\n",
            "Preprocessing train:  19%|█▉        | 85/442 [00:02<00:09, 36.71it/s]\n",
            "Preprocessing train:  20%|██        | 89/442 [00:03<00:11, 31.17it/s]\n",
            "Preprocessing train:  21%|██        | 93/442 [00:03<00:11, 29.35it/s]\n",
            "Preprocessing train:  22%|██▏       | 97/442 [00:03<00:11, 29.54it/s]\n",
            "Preprocessing train:  23%|██▎       | 101/442 [00:03<00:11, 30.38it/s]\n",
            "Preprocessing train:  24%|██▍       | 105/442 [00:03<00:12, 26.28it/s]\n",
            "Preprocessing train:  25%|██▍       | 109/442 [00:03<00:11, 28.63it/s]\n",
            "Preprocessing train:  26%|██▌       | 113/442 [00:04<00:11, 29.36it/s]\n",
            "Preprocessing train:  26%|██▋       | 117/442 [00:04<00:10, 31.54it/s]\n",
            "Preprocessing train:  27%|██▋       | 121/442 [00:04<00:09, 32.91it/s]\n",
            "Preprocessing train:  29%|██▉       | 128/442 [00:04<00:07, 40.54it/s]\n",
            "Preprocessing train:  30%|███       | 134/442 [00:04<00:07, 41.36it/s]\n",
            "Preprocessing train:  31%|███▏      | 139/442 [00:04<00:07, 40.64it/s]\n",
            "Preprocessing train:  33%|███▎      | 145/442 [00:04<00:06, 43.67it/s]\n",
            "Preprocessing train:  34%|███▍      | 150/442 [00:04<00:06, 43.87it/s]\n",
            "Preprocessing train:  35%|███▌      | 155/442 [00:05<00:08, 35.84it/s]\n",
            "Preprocessing train:  36%|███▌      | 159/442 [00:05<00:10, 28.27it/s]\n",
            "Preprocessing train:  37%|███▋      | 163/442 [00:05<00:09, 30.29it/s]\n",
            "Preprocessing train:  38%|███▊      | 167/442 [00:05<00:08, 30.85it/s]\n",
            "Preprocessing train:  39%|███▊      | 171/442 [00:05<00:09, 27.74it/s]\n",
            "Preprocessing train:  39%|███▉      | 174/442 [00:05<00:09, 27.14it/s]\n",
            "Preprocessing train:  40%|████      | 177/442 [00:05<00:10, 26.35it/s]\n",
            "Preprocessing train:  41%|████      | 180/442 [00:06<00:09, 26.51it/s]\n",
            "Preprocessing train:  41%|████▏     | 183/442 [00:06<00:10, 25.76it/s]\n",
            "Preprocessing train:  42%|████▏     | 186/442 [00:06<00:11, 21.70it/s]\n",
            "Preprocessing train:  43%|████▎     | 189/442 [00:06<00:11, 21.40it/s]\n",
            "Preprocessing train:  44%|████▎     | 193/442 [00:06<00:10, 23.99it/s]\n",
            "Preprocessing train:  44%|████▍     | 196/442 [00:06<00:09, 24.81it/s]\n",
            "Preprocessing train:  45%|████▌     | 200/442 [00:06<00:09, 25.16it/s]\n",
            "Preprocessing train:  46%|████▌     | 203/442 [00:07<00:09, 25.33it/s]\n",
            "Preprocessing train:  47%|████▋     | 207/442 [00:07<00:08, 28.37it/s]\n",
            "Preprocessing train:  48%|████▊     | 210/442 [00:07<00:08, 27.49it/s]\n",
            "Preprocessing train:  48%|████▊     | 213/442 [00:07<00:08, 26.88it/s]\n",
            "Preprocessing train:  49%|████▉     | 217/442 [00:07<00:08, 27.88it/s]\n",
            "Preprocessing train:  50%|█████     | 221/442 [00:07<00:07, 30.50it/s]\n",
            "Preprocessing train:  51%|█████     | 225/442 [00:07<00:09, 23.57it/s]\n",
            "Preprocessing train:  52%|█████▏    | 228/442 [00:07<00:08, 24.15it/s]\n",
            "Preprocessing train:  52%|█████▏    | 232/442 [00:08<00:08, 25.48it/s]\n",
            "Preprocessing train:  53%|█████▎    | 236/442 [00:08<00:07, 26.87it/s]\n",
            "Preprocessing train:  54%|█████▍    | 240/442 [00:08<00:06, 29.20it/s]\n",
            "Preprocessing train:  55%|█████▌    | 244/442 [00:08<00:06, 31.22it/s]\n",
            "Preprocessing train:  56%|█████▌    | 248/442 [00:08<00:06, 28.69it/s]\n",
            "Preprocessing train:  57%|█████▋    | 252/442 [00:08<00:06, 30.03it/s]\n",
            "Preprocessing train:  58%|█████▊    | 256/442 [00:08<00:06, 29.10it/s]\n",
            "Preprocessing train:  59%|█████▊    | 259/442 [00:09<00:06, 28.88it/s]\n",
            "Preprocessing train:  59%|█████▉    | 262/442 [00:09<00:06, 27.89it/s]\n",
            "Preprocessing train:  60%|█████▉    | 265/442 [00:09<00:06, 27.99it/s]\n",
            "Preprocessing train:  61%|██████    | 270/442 [00:09<00:05, 30.39it/s]\n",
            "Preprocessing train:  62%|██████▏   | 274/442 [00:09<00:05, 31.54it/s]\n",
            "Preprocessing train:  63%|██████▎   | 278/442 [00:09<00:05, 27.89it/s]\n",
            "Preprocessing train:  64%|██████▍   | 282/442 [00:09<00:05, 30.09it/s]\n",
            "Preprocessing train:  65%|██████▍   | 286/442 [00:09<00:05, 29.66it/s]\n",
            "Preprocessing train:  66%|██████▌   | 290/442 [00:10<00:04, 31.01it/s]\n",
            "Preprocessing train:  67%|██████▋   | 294/442 [00:10<00:04, 30.27it/s]\n",
            "Preprocessing train:  67%|██████▋   | 298/442 [00:10<00:04, 30.64it/s]\n",
            "Preprocessing train:  69%|██████▊   | 303/442 [00:10<00:04, 34.15it/s]\n",
            "Preprocessing train:  69%|██████▉   | 307/442 [00:10<00:04, 27.38it/s]\n",
            "Preprocessing train:  70%|███████   | 311/442 [00:10<00:04, 29.06it/s]\n",
            "Preprocessing train:  71%|███████▏  | 316/442 [00:10<00:03, 31.60it/s]\n",
            "Preprocessing train:  73%|███████▎  | 321/442 [00:11<00:03, 32.11it/s]\n",
            "Preprocessing train:  74%|███████▎  | 325/442 [00:11<00:03, 31.43it/s]\n",
            "Preprocessing train:  74%|███████▍  | 329/442 [00:11<00:04, 25.71it/s]\n",
            "Preprocessing train:  75%|███████▌  | 333/442 [00:11<00:03, 28.36it/s]\n",
            "Preprocessing train:  76%|███████▋  | 338/442 [00:11<00:03, 32.12it/s]\n",
            "Preprocessing train:  78%|███████▊  | 343/442 [00:11<00:02, 36.08it/s]\n",
            "Preprocessing train:  79%|███████▊  | 347/442 [00:11<00:02, 33.87it/s]\n",
            "Preprocessing train:  79%|███████▉  | 351/442 [00:12<00:02, 32.10it/s]\n",
            "Preprocessing train:  80%|████████  | 355/442 [00:12<00:02, 33.93it/s]\n",
            "Preprocessing train:  81%|████████  | 359/442 [00:12<00:02, 34.78it/s]\n",
            "Preprocessing train:  82%|████████▏ | 363/442 [00:12<00:02, 28.42it/s]\n",
            "Preprocessing train:  83%|████████▎ | 367/442 [00:12<00:02, 28.14it/s]\n",
            "Preprocessing train:  84%|████████▍ | 371/442 [00:12<00:02, 26.34it/s]\n",
            "Preprocessing train:  85%|████████▍ | 374/442 [00:12<00:02, 26.89it/s]\n",
            "Preprocessing train:  86%|████████▌ | 378/442 [00:12<00:02, 28.96it/s]\n",
            "Preprocessing train:  86%|████████▋ | 382/442 [00:13<00:02, 29.26it/s]\n",
            "Preprocessing train:  87%|████████▋ | 386/442 [00:13<00:01, 28.31it/s]\n",
            "Preprocessing train:  88%|████████▊ | 391/442 [00:13<00:01, 30.19it/s]\n",
            "Preprocessing train:  89%|████████▉ | 395/442 [00:13<00:01, 28.64it/s]\n",
            "Preprocessing train:  90%|█████████ | 398/442 [00:13<00:01, 23.58it/s]\n",
            "Preprocessing train:  91%|█████████ | 401/442 [00:13<00:01, 24.36it/s]\n",
            "Preprocessing train:  92%|█████████▏| 406/442 [00:13<00:01, 28.39it/s]\n",
            "Preprocessing train:  93%|█████████▎| 411/442 [00:14<00:00, 32.23it/s]\n",
            "Preprocessing train:  94%|█████████▍| 415/442 [00:14<00:00, 32.23it/s]\n",
            "Preprocessing train:  95%|█████████▍| 419/442 [00:14<00:00, 29.67it/s]\n",
            "Preprocessing train:  96%|█████████▌| 423/442 [00:14<00:00, 25.27it/s]\n",
            "Preprocessing train:  96%|█████████▋| 426/442 [00:14<00:00, 24.94it/s]\n",
            "Preprocessing train:  97%|█████████▋| 429/442 [00:14<00:00, 25.26it/s]\n",
            "Preprocessing train:  98%|█████████▊| 433/442 [00:14<00:00, 28.31it/s]\n",
            "Preprocessing train:  99%|█████████▊| 436/442 [00:15<00:00, 25.82it/s]\n",
            "Preprocessing train: 100%|█████████▉| 440/442 [00:15<00:00, 29.04it/s]\n",
            "Preprocessing train: 100%|██████████| 442/442 [00:15<00:00, 28.94it/s]\n",
            "\n",
            "Preprocessing dev:   0%|          | 0/48 [00:00<?, ?it/s]\n",
            "Preprocessing dev:   4%|▍         | 2/48 [00:00<00:02, 18.29it/s]\n",
            "Preprocessing dev:  10%|█         | 5/48 [00:00<00:01, 21.61it/s]\n",
            "Preprocessing dev:  17%|█▋        | 8/48 [00:00<00:01, 22.04it/s]\n",
            "Preprocessing dev:  25%|██▌       | 12/48 [00:00<00:01, 26.60it/s]\n",
            "Preprocessing dev:  33%|███▎      | 16/48 [00:00<00:01, 25.78it/s]\n",
            "Preprocessing dev:  44%|████▍     | 21/48 [00:00<00:00, 31.31it/s]\n",
            "Preprocessing dev:  52%|█████▏    | 25/48 [00:00<00:00, 26.44it/s]\n",
            "Preprocessing dev:  58%|█████▊    | 28/48 [00:01<00:00, 26.58it/s]\n",
            "Preprocessing dev:  69%|██████▉   | 33/48 [00:01<00:00, 31.87it/s]\n",
            "Preprocessing dev:  77%|███████▋  | 37/48 [00:01<00:00, 28.17it/s]\n",
            "Preprocessing dev:  85%|████████▌ | 41/48 [00:01<00:00, 28.70it/s]\n",
            "Preprocessing dev:  94%|█████████▍| 45/48 [00:01<00:00, 29.00it/s]\n",
            "Preprocessing dev: 100%|██████████| 48/48 [00:01<00:00, 27.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# Used for obtaining the training data\n",
        "! python ./preprocessing/download_wordvecs.py --download_dir ./data\n",
        "! python ./preprocessing/squad_preprocess.py --data_dir ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe88d2",
      "metadata": {},
      "source": [
        "From the paper:\n",
        "\"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent\n",
        "units, maxout layers, and linear layers. All LSTMs have randomly initialized parameters and an\n",
        "initial state of zero. Sentinel vectors are randomly initialized and optimized during training. For\n",
        "the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool size of\n",
        "16. We use dropout to regularize our network during training (Srivastava et al., 2014), and optimize\n",
        "the model using ADAM (Kingma & Ba, 2014). All models are implemented and trained with\n",
        "Chainer (Tokui et al., 2015).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6aba24a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import DynamicCoattentionNW\n",
        "import chainer as ch\n",
        "import chainer.functions as F\n",
        "\n",
        "\n",
        "max_seq_length = 600\n",
        "\n",
        "hid_state_size = 200\n",
        "\n",
        "dyn_dec_max_it = 4\n",
        "maxout_pool_size = 16\n",
        "\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1937fde7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GLoVE vectors from file: ./data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2196017/2196017 [01:48<00:00, 20200.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2196018, 300)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from preprocessing.vocab import get_glove\n",
        "\n",
        "glove_path = \"./data/glove.840B.300d.txt\"\n",
        "glove_vector_size = 300\n",
        "emb_mat, word2id, id2word = get_glove(glove_path, glove_vector_size)\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "print(emb_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5a42c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Refilling batches...\n",
            "Refilling batches took 0.07267332077026367 seconds\n",
            "[ 16  69 131 130]\n",
            "[ 17  71  71 130]\n",
            "[[92 93]\n",
            " [ 7  8]\n",
            " [27 28]\n",
            " [71 71]]\n"
          ]
        }
      ],
      "source": [
        "# %load_ext autoreload\n",
        "\n",
        "# %autoreload 2\n",
        "\n",
        "# from data_batcher import get_batch_generator\n",
        "# batchsize = 4\n",
        "# # batchsize = 128\n",
        "\n",
        "# train_context_path = \"./data/train.context\"\n",
        "# train_qn_path = \"./data/train.question\"\n",
        "# train_ans_path = \"./data/train.answer\"\n",
        "# train_span_path = \"./data/train.span\"\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "# #temp\n",
        "# from model import DocAndQuesEncoder, CoattentionEncoder, DynamicPointingDecoder, DynamicCoattentionNW\n",
        "# # enc = DocAndQuesEncoder(emb_mat, dropout, hid_state_size)\n",
        "# # enc2 = CoattentionEncoder(dropout, hid_state_size)\n",
        "# # dec = DynamicPointingDecoder(dropout, hid_state_size, maxout_pool_size, dyn_dec_max_it)\n",
        "# model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# for batch in batch_gen:\n",
        "#     s, e = model.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # D, Q = enc.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # U = enc2.forward(D, Q)\n",
        "#     # s, e = dec.forward(U)\n",
        "#     print(s)\n",
        "#     print(e)\n",
        "\n",
        "\n",
        "#     print(batch.ans_span)\n",
        "#     # print(batch.ans_tokens)   \n",
        "#     break\n",
        "\n",
        "# # train_iter = ch.iterators.SerialIterator(train, batchsize)\n",
        "# # test_iter = ch.iterators.SerialIterator(test, batchsize, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6bf5301c",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f88519d1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<chainer.optimizers.adam.Adam at 0x29ddd0f2610>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam()\n",
        "optimizer.setup(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "afff63e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def a_loss_function(prediction, ground_truth):\n",
        "    s_p, e_p = prediction\n",
        "    s_t = ground_truth[:,0].astype('f')\n",
        "    e_t = ground_truth[:,1].astype('f')\n",
        "\n",
        "    start_loss = F.mean_squared_error(s_p.astype('f'), s_t)\n",
        "    end_loss = F.mean_squared_error(e_p.astype('f'), e_t)\n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcac50e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 0.606569766998291 seconds\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "Out of memory allocating 23,040,000 bytes (allocated so far: 17,320,153,088 bytes).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m c_seq \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mVariable(cuda\u001b[38;5;241m.\u001b[39mto_gpu(batch\u001b[38;5;241m.\u001b[39mcontext_ids))\n\u001b[0;32m     22\u001b[0m q_seq \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mVariable(cuda\u001b[38;5;241m.\u001b[39mto_gpu(batch\u001b[38;5;241m.\u001b[39mqn_ids))\n\u001b[1;32m---> 23\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# prediction = model(batch.context_ids, batch.qn_ids)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate the loss with softmax_cross_entropy\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m a_loss_function(prediction, cuda\u001b[38;5;241m.\u001b[39mto_gpu(batch\u001b[38;5;241m.\u001b[39mans_span))\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\link.py:287\u001b[0m, in \u001b[0;36mLink.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# forward is implemented in the child classes\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Call forward_postprocess hook\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
            "File \u001b[1;32mc:\\Users\\temp\\OneDrive - TU Wien\\Studium TU\\MSem3\\DLNLP\\DLNLP-project\\model.py:198\u001b[0m, in \u001b[0;36mDynamicCoattentionNW.forward\u001b[1;34m(self, d, q)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, d, q):\n\u001b[1;32m--> 198\u001b[0m     D, Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocQuesEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoAttEncoder(D, Q)\n\u001b[0;32m    202\u001b[0m     s, e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(U)\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\link.py:287\u001b[0m, in \u001b[0;36mLink.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# forward is implemented in the child classes\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Call forward_postprocess hook\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
            "File \u001b[1;32mc:\\Users\\temp\\OneDrive - TU Wien\\Studium TU\\MSem3\\DLNLP\\DLNLP-project\\model.py:33\u001b[0m, in \u001b[0;36mDocAndQuesEncoder.forward\u001b[1;34m(self, x_D, x_Q, hx_D, cx_D, hx_Q, cx_Q)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_D, x_Q, hx_D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cx_D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hx_Q\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cx_Q\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 33\u001b[0m     x_D_emb \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_D\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# nstep LSTM needs it split into a list\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     x_D_emb_ \u001b[38;5;241m=\u001b[39m [x_D_emb[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_D_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\link.py:287\u001b[0m, in \u001b[0;36mLink.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# forward is implemented in the child classes\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Call forward_postprocess hook\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\links\\connection\\embed_id.py:88\u001b[0m, in \u001b[0;36mEmbedID.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts the word embedding of given IDs.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membed_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_label\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\functions\\connection\\embed_id.py:168\u001b[0m, in \u001b[0;36membed_id\u001b[1;34m(x, W, ignore_label)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_id\u001b[39m(x, W, ignore_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Efficient linear function for one-hot input.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    This function implements so called *word embeddings*. It takes two\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEmbedIDFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_label\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\function_node.py:334\u001b[0m, in \u001b[0;36mFunctionNode.apply\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(in_data)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;66;03m# In normal case, simply run the forward method.\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Check for output array types\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\functions\\connection\\embed_id.py:47\u001b[0m, in \u001b[0;36mEmbedIDFunction.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_label)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mwhere(mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;241m0\u001b[39m, W[xp\u001b[38;5;241m.\u001b[39mwhere(mask, \u001b[38;5;241m0\u001b[39m, x)]),\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m,\n",
            "File \u001b[1;32mcupy\\_core\\core.pyx:1521\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__getitem__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:42\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._ndarray_getitem\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\core.pyx:829\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.take\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:131\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._ndarray_take\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:815\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._take\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\core.pyx:132\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\_core\\core.pyx:220\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:740\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1426\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1447\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1118\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1139\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1384\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mcupy\\cuda\\memory.pyx:1387\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: Out of memory allocating 23,040,000 bytes (allocated so far: 17,320,153,088 bytes)."
          ]
        }
      ],
      "source": [
        "from data_batcher import get_batch_generator\n",
        "from chainer.backends import cuda\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# batchsize = 32\n",
        "# batchsize = 128\n",
        "batchsize = 64\n",
        "max_epoch = 1\n",
        "\n",
        "train_file_lines = 86307\n",
        "\n",
        "train_context_path = \"./data/train.context\"\n",
        "train_qn_path = \"./data/train.question\"\n",
        "train_ans_path = \"./data/train.answer\"\n",
        "train_span_path = \"./data/train.span\"\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "model.to_gpu()\n",
        "for epoch in range(max_epoch):\n",
        "    batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "    for batch in tqdm(batch_gen, total=train_file_lines/batchsize):\n",
        "    # for batch in batch_gen:\n",
        "        tic = time.time()\n",
        "        # Calculate the prediction of the network\n",
        "        c_seq = ch.Variable(cuda.to_gpu(batch.context_ids))\n",
        "        q_seq = ch.Variable(cuda.to_gpu(batch.qn_ids))\n",
        "        prediction = model(c_seq, q_seq)\n",
        "        # prediction = model(batch.context_ids, batch.qn_ids)\n",
        "\n",
        "        # Calculate the loss with softmax_cross_entropy\n",
        "        loss = a_loss_function(prediction, cuda.to_gpu(batch.ans_span))\n",
        "\n",
        "        # Calculate the gradients in the network\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update all the trainable parameters\n",
        "        optimizer.update()\n",
        "\n",
        "        # print(\"Batch done\")\n",
        "        toc = time.time()\n",
        "        print(\"Current batch took {} seconds\".format(toc-tic))\n",
        "        print(loss)\n",
        "\n",
        "    ch.serializers.save_npz('DCANW_E{}.model'.format(epoch), model)\n",
        "    \n",
        "\n",
        "    #  # Display the training loss\n",
        "    #     print('epoch:{} train_loss:{:.04f} '.format(\n",
        "    #         epoch, ))\n",
        "\n",
        "    #     test_losses = []\n",
        "    #     test_accuracies = []\n",
        "    #     for test_batch in test_iter:\n",
        "    #         image_test, target_test = concat_examples(test_batch, gpu_id)\n",
        "\n",
        "    #         # Forward the test data\n",
        "    #         prediction_test = model(image_test)\n",
        "\n",
        "    #         # Calculate the loss\n",
        "    #         loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
        "    #         test_losses.append(to_cpu(loss_test.array))\n",
        "\n",
        "    #         # Calculate the accuracy\n",
        "    #         accuracy = F.accuracy(prediction_test, target_test)\n",
        "    #         accuracy.to_cpu()\n",
        "    #         test_accuracies.append(accuracy.array)\n",
        "\n",
        "    #     test_iter.reset()\n",
        "\n",
        "    #     print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
        "    #         np.mean(test_losses), np.mean(test_accuracies)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea519ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the updater, using the optimizer\n",
        "updater = ch.training.StandardUpdater(train_iter, optimizer, device=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50999c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up a trainer\n",
        "max_epoch = 10\n",
        "trainer = ch.training.Trainer(updater, (max_epoch, 'epoch'), out='result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cd42e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "#see https://docs.chainer.org/en/stable/guides/serializers.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLNLP-Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
