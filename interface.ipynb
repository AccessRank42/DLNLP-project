{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f8df10b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Used for obtaining the training data\n",
        "# ! python ./preprocessing/download_wordvecs.py --download_dir ./data\n",
        "# ! python ./preprocessing/squad_preprocess.py --data_dir ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe88d2",
      "metadata": {},
      "source": [
        "From the paper:\n",
        "\"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent\n",
        "units, maxout layers, and linear layers. All LSTMs have randomly initialized parameters and an\n",
        "initial state of zero. Sentinel vectors are randomly initialized and optimized during training. For\n",
        "the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool size of\n",
        "16. We use dropout to regularize our network during training (Srivastava et al., 2014), and optimize\n",
        "the model using ADAM (Kingma & Ba, 2014). All models are implemented and trained with\n",
        "Chainer (Tokui et al., 2015).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6aba24a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\temp\\anaconda3\\envs\\DLNLP-Project\\lib\\site-packages\\chainer\\_environment_check.py:72: UserWarning: \n",
            "--------------------------------------------------------------------------------\n",
            "CuPy (cupy) version 12.3.0 may not be compatible with this version of Chainer.\n",
            "Please consider installing the supported version by running:\n",
            "  $ pip install 'cupy>=7.7.0,<8.0.0'\n",
            "\n",
            "See the following page for more details:\n",
            "  https://docs.cupy.dev/en/latest/install.html\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  warnings.warn(msg.format(\n"
          ]
        }
      ],
      "source": [
        "from model import DynamicCoattentionNW\n",
        "import chainer as ch\n",
        "import chainer.functions as F\n",
        "\n",
        "\n",
        "max_seq_length = 600\n",
        "\n",
        "hid_state_size = 200\n",
        "\n",
        "dyn_dec_max_it = 4\n",
        "maxout_pool_size = 16\n",
        "\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1937fde7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GLoVE vectors from file: ./data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2196017/2196017 [01:48<00:00, 20281.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2196018, 300)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from preprocessing.vocab import get_glove\n",
        "\n",
        "glove_path = \"./data/glove.840B.300d.txt\"\n",
        "glove_vector_size = 300\n",
        "emb_mat, word2id, id2word = get_glove(glove_path, glove_vector_size)\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "print(emb_mat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e5a42c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "\n",
        "# %autoreload 2\n",
        "\n",
        "# from data_batcher import get_batch_generator\n",
        "# batchsize = 4\n",
        "# # batchsize = 128\n",
        "\n",
        "# train_context_path = \"./data/train.context\"\n",
        "# train_qn_path = \"./data/train.question\"\n",
        "# train_ans_path = \"./data/train.answer\"\n",
        "# train_span_path = \"./data/train.span\"\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "# #temp\n",
        "# from model import DocAndQuesEncoder, CoattentionEncoder, DynamicPointingDecoder, DynamicCoattentionNW\n",
        "# # enc = DocAndQuesEncoder(emb_mat, dropout, hid_state_size)\n",
        "# # enc2 = CoattentionEncoder(dropout, hid_state_size)\n",
        "# # dec = DynamicPointingDecoder(dropout, hid_state_size, maxout_pool_size, dyn_dec_max_it)\n",
        "# model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# for batch in batch_gen:\n",
        "#     s, e = model.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # D, Q = enc.forward(batch.context_ids, batch.qn_ids)\n",
        "#     # U = enc2.forward(D, Q)\n",
        "#     # s, e = dec.forward(U)\n",
        "#     print(s)\n",
        "#     print(e)\n",
        "\n",
        "\n",
        "#     print(batch.ans_span)\n",
        "#     # print(batch.ans_tokens)   \n",
        "#     break\n",
        "\n",
        "# # train_iter = ch.iterators.SerialIterator(train, batchsize)\n",
        "# # test_iter = ch.iterators.SerialIterator(test, batchsize, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "afff63e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def a_loss_function(prediction, ground_truth):\n",
        "    s_p, e_p = prediction\n",
        "    s_t = ground_truth[:,0].astype('f')\n",
        "    e_t = ground_truth[:,1].astype('f')\n",
        "\n",
        "    start_loss = F.mean_squared_error(s_p.astype('f'), s_t)\n",
        "    end_loss = F.mean_squared_error(e_p.astype('f'), e_t)\n",
        "    total_loss = start_loss + end_loss\n",
        "\n",
        "    if (s_p>e_p).any(): #penalize impossible start > end predictions\n",
        "        total_loss *= 2\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f88519d1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<model.DynamicCoattentionNW at 0x1ffc54f6a30>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = DynamicCoattentionNW(max_seq_length, hid_state_size, dyn_dec_max_it, maxout_pool_size, dropout, emb_mat)\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam()\n",
        "optimizer.setup(model)\n",
        "\n",
        "model.to_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcac50e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1348.546875 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.2815513610839844 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 100/1348.546875 [02:03<26:43,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "558034.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 160/1348.546875 [03:13<23:04,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.1703357696533203 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 200/1348.546875 [04:02<24:36,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "470720.71875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 300/1348.546875 [06:01<22:21,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "503531.15625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▎       | 320/1348.546875 [06:24<20:14,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.1794793605804443 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|██▉       | 400/1348.546875 [08:06<22:44,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "508498.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 480/1348.546875 [09:45<17:30,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.7500123977661133 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 500/1348.546875 [10:13<20:34,  1.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "472331.34375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 600/1348.546875 [12:20<16:21,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448070.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 640/1348.546875 [13:10<14:22,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.9687426090240479 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 700/1348.546875 [14:29<14:39,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "487435.15625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 800/1348.546875 [16:35<12:21,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "497031.6875\n",
            "Refilling batches...\n",
            "Refilling batches took 1.925781011581421 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 900/1348.546875 [18:41<09:36,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "489301.78125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 960/1348.546875 [19:52<07:45,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 1.1851739883422852 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 1000/1348.546875 [20:41<07:29,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391557.4375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 1100/1348.546875 [22:40<05:20,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "481671.1875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 1120/1348.546875 [23:07<05:25,  1.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refilling batches...\n",
            "Refilling batches took 2.0009267330169678 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 1172/1348.546875 [24:12<03:32,  1.20s/it]"
          ]
        }
      ],
      "source": [
        "from data_batcher import get_batch_generator\n",
        "from chainer.backends import cuda\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# batchsize = 16\n",
        "# batchsize = 32\n",
        "batchsize = 64\n",
        "max_epoch = 10\n",
        "\n",
        "train_file_lines = 86307\n",
        "\n",
        "train_context_path = \"./data/train.context\"\n",
        "train_qn_path = \"./data/train.question\"\n",
        "train_ans_path = \"./data/train.answer\"\n",
        "train_span_path = \"./data/train.span\"\n",
        "\n",
        "# batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    batch_gen = get_batch_generator(word2id, train_context_path, train_qn_path, train_span_path, batchsize, max_seq_length, max_seq_length, discard_long=True)\n",
        "    batch_id = 0\n",
        "    for batch in tqdm(batch_gen, total=train_file_lines/batchsize):\n",
        "        batch_id += 1\n",
        "    # for batch in batch_gen:\n",
        "        # tic = time.time()\n",
        "        # Calculate the prediction of the network\n",
        "        c_seq = ch.Variable(cuda.to_gpu(batch.context_ids))\n",
        "        q_seq = ch.Variable(cuda.to_gpu(batch.qn_ids))\n",
        "        prediction = model(c_seq, q_seq)\n",
        "        # prediction = model(batch.context_ids, batch.qn_ids)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = a_loss_function(prediction, cuda.to_gpu(batch.ans_span))\n",
        "\n",
        "        # Calculate the gradients in the network\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update all the trainable parameters\n",
        "        optimizer.update()\n",
        "\n",
        "        # print(\"Batch done\")\n",
        "        # toc = time.time()\n",
        "        # print(\"Current batch took {} seconds\".format(toc-tic))\n",
        "        if batch_id % 100 == 0:\n",
        "            print(loss.item())\n",
        "\n",
        "    ch.serializers.save_npz('DCANW_E{}.model'.format(epoch), model)\n",
        "    \n",
        "\n",
        "    #  # Display the training loss\n",
        "    #     print('epoch:{} train_loss:{:.04f} '.format(\n",
        "    #         epoch, ))\n",
        "\n",
        "    #     test_losses = []\n",
        "    #     test_accuracies = []\n",
        "    #     for test_batch in test_iter:\n",
        "    #         image_test, target_test = concat_examples(test_batch, gpu_id)\n",
        "\n",
        "    #         # Forward the test data\n",
        "    #         prediction_test = model(image_test)\n",
        "\n",
        "    #         # Calculate the loss\n",
        "    #         loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
        "    #         test_losses.append(to_cpu(loss_test.array))\n",
        "\n",
        "    #         # Calculate the accuracy\n",
        "    #         accuracy = F.accuracy(prediction_test, target_test)\n",
        "    #         accuracy.to_cpu()\n",
        "    #         test_accuracies.append(accuracy.array)\n",
        "\n",
        "    #     test_iter.reset()\n",
        "\n",
        "    #     print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
        "    #         np.mean(test_losses), np.mean(test_accuracies)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea519ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create the updater, using the optimizer\n",
        "# updater = ch.training.StandardUpdater(train_iter, optimizer, device=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50999c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set up a trainer\n",
        "# max_epoch = 10\n",
        "# trainer = ch.training.Trainer(updater, (max_epoch, 'epoch'), out='result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cd42e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "#see https://docs.chainer.org/en/stable/guides/serializers.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLNLP-Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
